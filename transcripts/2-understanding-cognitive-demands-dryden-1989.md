# 2. Understanding cognitive demands & goal conflicts; Dryden Air Ontario Crash, 1989

Welcome to seminar number two: understanding cognitive demands and the Dryden
Air Ontario crash in March of 1989. The thing we should begin with first is
questions that people have from previous session. Are there things that you
want to talk about or concerns that you have or issues that have come up,
things that you're now thinking about or things that you want to bring up for
attention or discussion?

Anything about or things that you want to bring up for attention or discussion. Okay. That's very reassuring. You've had a chance at least to look at some of the materials here and get a handle on the range of things and the range of things in the sponsor is quite large. That extends all the way from.

From very theoretical bits of stuff that looked really complicated to very concrete things like checklists to as far North as Winnipeg of Manitoba and as far South as the Micronesian islands. And so it's a kind of a global experience. And the goal of the seminars to put this all together in a way that makes some sense for you.

And it gets you a step closer to thinking about cognitive tasks and the cognitive domain that you're trying to enter. So let's first begin with the look at woods and the paper from 1987. Mapping cognitive demands in a complex problem solving worlds. And this is a an important paper, perhaps not a landmark paper it's been it's been I think not as widely used perhaps as the Not as widely used as the cognitive systems, engineering paper, new wine and new bottles, but it's from the same good ditch. And it's actually quite good for us because in a way it captures the sort of state of things. In 1987, since 1987, things have changed. People have gotten more sophisticated and things have moved around, but that's the first thing that you would notice that when you're looking at this paper is that they're still thinking about nuclear power plants, and they're still trying to figure out how to own their power plant and examples that they take a language is a language basically of control rooms.

No

control rooms and interfaces

control rooms interfaces to the nuclear power plant.

And the human operators

who are getting this information from the control room setting, but you can't see the power plant directly because it's not visible to them. We were trying to get the other and who are engaged in this complex process of trying to figure out What's wrong.

What's it doing?

What's available.

That is one of the things that I, what are the nods or levers that I can grab and twist and pull to make something happen and a lot of stuff. What are. The pluses and minuses of doing X one X, two or so on and so forth. These guys are the basic model of what's happening here is that the plant people are engaged in some sort of process control.

And that there's this complex and dangerous thing out here,

but that it's really quite separate from them. And there's this control room set up displays and indicators and knobs and dials and all that sort of stuff. That's what they see that from about that indicate supplanted from this. They, we got to the idea last time that in order to answer these questions, what these guys really do is they develop some sort of mental model

of what's happening and in their mental model,

they understand that there's some sort of problem. And from the mental model that they're running in their head, it's like a little simulator that's going on in their head. They are able to conclude from this that we should be able to see on the displays, a particular pattern

pattern.

It looks something like this that is, they have.

In order to be, are you able to answer this question? What they're trying to do in their heads is actually to run some sort of mental model of the plant and to develop in this model a kind of flaw or think about a particular kind of flaw and say that if that flaw were present, then I would see this particular pattern of instruments.

These controls would be remissed as control here either. And I would see that they're in there looking at this to try and think about what the diagnostic problem is. Okay. It's very much the same problem that you have if your spouse calls you up and tells you that the car won't start, you're not there.

You can't do it. Do anything with it. You can't tell what's going on, but your spouse says the car will start. Can you say you know what happens when you turn the key? And they will say nothing happens and you think, okay, this is the battery and so on and so forth because you have a mental model of how that car works.

And you're able to run that mental model and look at the different kinds of places at which you could have different kinds of faults and with the different kinds of displays that you might have.

You can see different kinds of patterns in these instruments

that will tell you what's going on. And each one of these will be the sort of simple, simplistic theory. Each one of the different faults will be manifested by some different set of display and display features. And you'll be able to figure it out, right? Depending upon which one of these things is broken, we'll have these different sorts of things going on here.

And that's the basic idea that we got from that. And people are trying to figure out ways to let what woods and other people are doing. Is inset in some sense, trying to hold back the tide because what's happening in the wake of three mile Island is everyone and his uncle who thinks they have anything to do any everyone who believes that they have any insight at all.

Into human psychology or operators or plants or instruments for others is proposing solutions to keep three mile Island from happening again. So you've got just yards of pieces of paper, stacks of paper coming in to the nuclear regulatory commission and all the plant manufacturers and all the concerned interest groups solutions, essentially solutions to TMI.

And they're all based upon this model of what happened at TMI. They all have a model of what happened

and they all have an idea about how to design the control room.

They're all looking at ways to change the control room round in some way. So that later on, these guys don't get caught in the same problem, the very straightforward idea. They are not by the way, thinking very much about changing the plant. They're thinking about changing the control room.

And the reason is the plant, which costs several billion dollars has already been built. All we can do now is slap on other things to try and fix this. So it, they really see it as being a problem of getting the controls, right? So that the operators can figure this out and not make mistakes. And what's this saying in this paper, in both the woods homemaker, and that in one case it's woods it whole angle, and the other case it's whole mango and woods.

And this is because they flip the coin to decide who's going to be the number one and number two persons on these papers. And you'll see them doing this. Throughout their careers trying to do this. So there's no, no one knows who is the real source of the ideas here. It's just a keep changing your authors.

What they're trying to do is they're trying to say, look, there's gotta be some sort of orderly way of going about this because in fact, there are just way too many solutions out here that, and the solutions are not good in large part because the particular problem. Of TMI, that particular sequence that they had likely to be the thing that happens the next time and the people who are proposing all these solutions are basically people who say off, we have another alarm out here.

If we had another Visier over there, he had another thing over here, then we will solve the problem. And what these guys are saying is no, no way. You've got to rethink this. All. You got to think more about what is going on with the operator, not just building a set of displays. That cover all the different possible ways that the plan can be broken.

I'd rather ask yourself some questions about how is it that operators are doing these sorts of things and what are they doing? They're all process control questions that is, they're all imagined that the world is running truthfully, the plant is running

and then something happens. Click,  Terman trips, the reactor trips. A pipe breaks, but something happens. And then the process where it goes missing and they then get, have to start figuring things out very much the model, what happened to, to your mind, right? The world's working, everything's okay.

And then something happens and now we have to figure out what's wrong. This is the problem that everyone is trying to deal with. Very few people at this time are saying, Hey, wait, maybe this is not something that we like to be building. Maybe this is too complicated. Maybe this is something that nobody's going to ever be able to figure out.

Maybe he's got too many flaws and faults in it. And TMI, which is 1979 is followed in 1985 by the Davis Bessey event, which is the one that they analyzed in the paper and Davis Bessey and is a, is another plant, very similar to this plant it's sister. And what happens at Davis Bessey. Is another version of TMI.

They have the same accident. Again, they have a trip, they have the loss of coolant flow in the secondary side, they have issues about the reactor having too much heated at the open up the por V and so not became, just all the stuff that happened to PNI happens again, six years later and everybody goes, Holy cow, this is TMI again.

One of the problems with what happens at Davis Bessey is that knowing that it's TMI again, doesn't allow them to solve the problem. And there's a variety of reasons. It's a long, complex story that you can read about it to watch it. But th the important thing is that, although it's a kind of second TMI or TMI, number two, it's different enough that the kinds of things that the operators are trying to do are different.

In fact, they spend time trying to do things. That are not in the list of appropriate things to do when you're in TMI that came out of PMI, right after TMI, there was a lot of paper that came out and said, if this ever happens again, I want you to push this button. But when it happened again, they didn't immediately push them up and they did some other things.

And everybody was very concerned about that. People were really upset. You, we told you to push a button. Why didn't you push the button? The answer was complicated, but it had a lot to do with the fact that. Pushing the button would have had some undesired consequences and they thought they could fix the problem.

And they went out and fixed the problem in a short enough period of time that they were able to keep, to restore the plant operation without pushing the button. And everybody was really upset about this. Now it was clear that the operators were not willing to follow directions under any conditions.

You just couldn't make the rules strong enough to get everybody to do this. And the reason was because they're actually thinking about what's going on, what's wrong? What is it doing now? What is available for me to do? And what are the different consequences of doing these different things?

Because none of them are consequence street. They all have, all of them have side effects and bad effects and other issues that are associated with into that world step wasn't, whole Nagel. And they say, Oh, look, it's. This idea about trying to fashion solutions for the accident that happened in the past is the wrong idea because the accident that happens in the future is going to be different again than this either TMR Davis, Bessey, it's not going to repeat exactly the same way, and we're not clever enough to think of all the different ways that the system can fail.

We've demonstrated that with TMI and David's best seeds, some other plants. So what we really need to do is we really need to. Work on this business about how people are able to figure out models and figure out what's going on and how they test those sorts of things, how they answer these sorts of questions.

And this is why they are interested in mapping the cognitive demands and complex problem solving worlds. This is a complex problem solving world. The process was going along fine, bang it broke. And now they're trying to fix it. They're trying to figure out what's wrong and to fix it. And that's a set of cognitive demands that are facing the operator.

The operator is asked to do things, to figure out the answers to these sorts of questions, using the data that they can get from this control room set of displays. And what would send, hold angler saying is. You're spending too much time talking about the little details of each one of these different displays and not thinking enough about the bigger problem, which is that people have to be able to figure out what the point is doing and they have to figure out what the is the sequences of taking different actions are in particular.

They're really concerned about this because this issue about what happens if I do this, what happens. If I turn this on or turn this off, is that the key idea here? The purpose of these people is to keep the plant from melting down, but also to keep it from being destroyed or damaged. And they're trying to do that by looking at a whole bunch of different options.

And the question is how can they, how do they make these decisions and how do they select different kinds of choices and what woods in whole Nagel or whole neighborhood woods are saying is. Somehow you have to be able to add, look at what the demands are associated with answering these problems and assist people in solving those problems, rather than insisting upon knowing upon pretending that you know what to do, you've got to stop saying.

The designers of these control rooms are they get to stop saying you are the expert and you just want the operators to follow the directions. You have to start taking them seriously and say, what are the demands that these guys are facing? Not in control room terms, but in these larger church.

And what they want to do is they actually ask us in here to recognize. That we don't actually have a good language for this. That is this stuff here. How this is expressed, how do we say this? What they are doing, what is going on? We don't actually have a language at that time. 1987. There's no language to describe this stuff that they are doing.

They don't. We've taught we've reduced everything to concrete stuff. Turn this switch, close that valve, check this. If it's above that sets of instructions, but nothing to talk about this bigger stuff. And in particular, now after three mile Island, we all recognize that there's so much stuff out here.

There's so much data that in fact, just managing the data itself is one of the cognitive tasks. That is not only do you have to figure it out, you have to look at this world that's filled with signals and data and try and decide what's meaningful in that because some of the signals that you're getting are signals about the way the plant is failing.

And some of the signals you're getting are consequences of this failure, but there are secondary consequences and some of them are wrong. Data. Some of this stuff is not really known. Some of it's going to be. You remember the por V example, the light is on, but the valve is enclosed. So some of the data is actually false.

It's incorrect. They have to live in a world where data is not pure. And so there's uncertainty that's associated with it. So they want to know how people are going to manage these kinds of uncertainties. And so what they're trying to get us to do is to think about ways in which we might, instead of talking about what people ought to do.

Ask what it is that people are actually going to do. And they tell us explicitly on page two 58, they say to build a cognitive description of a complex world, the first hurdle was to escape from the language of the app. And what they really have done is they said, look, it's really hard to know what's going on inside people's heads.

We can't look inside your head, even if we were to. Take your head and cut your,  your skull off like this and lift it off and look inside. We still wouldn't see what you're thinking. And so it's really hard to get in here. And it's really hard to answer these questions in any kind of meaningful way of what we should be doing is we should be looking at what are the real demands that are flowing in the system.

Now, how do people solve the problems and what are they using to solve the problems, but what problems are there they required to solve? What is the demand that they are facing? What is the requirements for them looking mostly from the system side, towards the operator? That is that they're saying that the system is pretty sending problems for the operator to solve.

They're not talking about how offering yourself problems in this paper. They're talking about how do we characterize what the system is requiring? People, this is again, still very much focused on this control room view of the world. There's a fixed set of data. There's only so much. You can see you're looking at everything and this rule out, here's a nice static world.

Nobody's doing construction on the power plant. This time, nobody's building a new power plant. It's a power plant. That's been built that you understand. So your mental models are pretty well fixed for this. And so all we have to do is just tell you, figure out a way to explain what the real demands are.

And then we'll get to this part about figuring out what you ought to actually do. The important thing about this is that they immediately recognize that this is really hard to do.  They recognize that. In the end, although the, these, the manager coming from the world, they're perceived by the operator. And so it's really the operator's ability to formulate these sorts of questions and understand what's going on in terms of these questions.

So that matters. And so they aren't, they're stuck once again with having to go and look into the operators, see what's going on. And they, they admit that this is a really hard thing to do, but they give you a list of suggestions on page two 59. They say it's a big debate. It's hard to figure out there's lots of stuff.

That's complicated here. And then the second last paragraph they say as the analysis progresses, it helps the cognitive technologists to interpret answers to questions or observations and decide where to direct further investigation so that the domain they say, and then they give you the list of things to do here.

They say that. It doesn't matter how you do this. The approach is indifferent to the approaches is different to the particular sources of a domain information, which means basically we don't care where you find out what's going on. There's no best source of information. There's no perfect operator.

There's no, God's eye view of this. It doesn't matter. You can ask anybody you want, you can find a right specialist to talk to. Look at the right documents or analyses look empirically at how a problem is solved by sending one up in the simulator or watching people do this or putting the problem solver in a simulation setting interviewing people to perform the task.

People who specialties interests with the task. This is a pretty big list. Okay. And it's not very well formed. It's saying go out and talk to people, figure out what's going on. Does it give you a lot of time? It's not the kind of stuff that we would think of as extremely valuable.

They're saying, look, you have to figure this out. And somehow you can go out there and do some field studies or do some pardon me to people or ask somebody what the heck is going on when people are trying to solve these problems. And in fact, one of the remarkable things about the rest of the paper is that it doesn't really give you very much stuff about how that should actually be done.

It then slips on into it. The goal means analysis and what that means. And that's about the level of help that you're going to get from me. Gosh, you, aren't going to help you much more than that now, in part they're not going to help you much more than that because the system that they're actually looking at is a pretty fixed system control rooms.

Don't change very rapidly. The instruments you go in on Tuesday, the instruments are in exactly the same place they were on Monday. Okay. The plant, the thermodynamics, the way energy is transferred, all the rest of that stuff on Wednesday is going to be the same as it was the previous Sunday. There's very little change in this world.

So figuring this stuff out is going to be relatively easy. Although they talk about a dynamic world and dynamic dynamism being a problem. From an analytical standpoint, the world is pretty fixed. It's pretty nice. It's not like geologic, it's not like mountains and stuff nuclear power plant once it's there, it's pretty well there.

And the piping doesn't get changed overnight into some different set of connections. So they weave their hands a little bit about how Archie to do this and jump right into the snakes business was just a goal means now. And the idea of the goal means analysis

The idea of the goal means analysis is it is a classic Western approach to figuring out how systems work. It's a goal decomposition. And it says basically that there's a. There's a main goal out here

and that every goal has some means to accomplish that goal. And we're going to figure out what the goal is, and we're going to figure out what the means are. This is what they're saying. It's very simple. This is what we're trying to achieve. That might be something like we want the plants to move on to the core.

Should stay covered with water. Okay. That's a goal

and that's a very concrete, everybody understands this when you uncover the core bad things happen when they shouldn't of course over the water, that's a goal. It's irrespective of the other things in the power plant. That's the goal. It's a very high level goal, right? If you ask what will people be willing to let the goal be uncovered if it's a party night and everybody wants to go home early, the answer's no.

Okay. It's a really high level goal, right? So we've got goal one out here, which is keep the core coverage and then they'll ask what do you need to do that? And so then there'll be some means for accomplishing that goal. And those means will be. A variety of different things, but what you'll get in here are things like inflows of water in a different variety of ways to get water in there to keep the thing covered. And so one of the things like closing valves, That

water out, kind of concrete stuff, right? And you can make a list of these. You can get up, you can actually go through and say, this there's this for me. Now, the problem is that these means themselves are not things you can accomplish. You can't say I want an inflow of water and just have it appear you can't.

It's not, you're not God. And so you can't say, give me water and have it up here. You'd have to get your inflow of water from someplace, which means that there has to be down below this. Some other set of means for which this thing is actually a goal, but now we began to do the system decomposition process.

And what we're really doing is we're saying. What looks like a means to a very high level goal is actually another goal to another lower level of things which might be the emergency core. Okay. Okay. We have an emergency core cooling system. It creates an inflow of water into the reactor. If we turn that on, we're going to get an inflow of water.

Therefore, this is a means. To accomplish this goal. So this level is this level here is interesting because this becomes viewed from the top. It's a means and viewed from the bottom, it's a goal. Okay. It's just a question of which one of the perspectives you take. And even down here you'll be able to work out, Oh, gee, the emergency occur.

Core cooling system is may made out of a variety of different things. It's got a whole bunch of different pipes and pumps and motors and stuff, and somehow those things have to be turned on and operated. So there's some other things that have to happen down here that are, or means to the, to this, which treat this as a goal.

And you could keep doing this. And if you do it long enough, what you're going to end up with down here is somewhere you're going to have. Something that looks like physical water faucet. And you're going to point at this and you're going to say,

if you turn this water will come out. That's what I'm talking about. It's that, that flush it right there. That valve, that fig that mechanical object, which I go to and I do this on, and that is. The means by which I accomplished the goals up above it. All right. This is the goal means hierarchy. It's the core idea of all analysis of these big systems.

It's not very much different than a standard idea of decomposition that you get in most analytic processes in Western thought. Every Western. Kind of notion about the analysis of systems involves looking at subsystems and other subsystems, but here what's significant about it is that it's described in terms of,

and the means are to a particular goal. And this is different because it is fundamentally what the word is.  logical.

Which is a philosophical way of saying has a purpose

that is we don't have these goals and means because they are inherent in the technology. We have these goals and these, because we have a purpose for doing what we're doing and this high-level hole to essentially keep the reactor safe. Requires us to keep that thing covered with water and all the rest of the things that flow from this are for that purpose.

It's all purposeful. It's all done for the purpose. It's not done. We're not tracing out the logical connections between things in the sense that we are tracing out the relationship between DNA and RNA and proteins and other things like that. We're not saying this is the religion. What we're saying is.

This is the series of purposes for which we have built the system and the way it is running. And in this analysis, we can do this as far as we want until we finally ground out, down here in concrete stuff. And Rasmussen would what Rasmussen does later. And you'll see this in his 1986 work.

And since then, as he points out that this. Kind of thing extends quite a ways up because the goal of having a power plant is on the one hand to provide electricity to the society. And the goal of electricity to the society is to allow people to be functional and live a good life and so on.

And Rasmussen says, look, we're mostly concerned about starting out here and working our way down. That's the Wood's view. Rasmussen turns this round goes way up the top. He says, wait a minute, we've got to really think about all of these things, because it turns out that a map of these shows that there are goals that are in conflict.

And so you could have a situation where you have a goal.

Let's take this goal as one. Reactor

for use not safe, save the reactor so that it doesn't cover it. It doesn't create a nuclear meltdown and all the problems, but we, it's a very expensive piece of machinery. It's cost a lot of money and we bought this thing and we paid for it and now we need to keep using it to get the payoff. So it's a valuable thing. We don't want to throw this away. You have to treat this with care and it turns out that saving the reactor for you, which means that there are certain things that you want probably not to do. And what, what of course will happen is that at some point there will be some place where there'll be a conflict back and forth between two goals, where one goal, the accomplishment of one goal of structure and other or gets in the way of them.

Or where the accomplishment of one goal depends upon another. There are two kinds of relationship. You'd have one, which is an obstruction. If I do this, then I foreclose the possibility of doing that. And another one, which is in order to do this, I need to actually do these other things as well.

Because some of these things, you can have multiple entries into this, right? There can be multiple means to achieving goals. In fact, probably all goals have multiple needs.

And so you begin to build this map and what you've got is lots and lots of boxes with their goals and means at the same time and the way that you, the way that you solve a conflict between two goals is you look up. If it looks like two goals are. All right, this are in conflict with each other. You to figure out what to do.

You look up and the reactor. So up the list. So saving the reactor for use as a good goal, but it's not as good as gold as preventing the reactor from melting down. I'd rather have a reactor. That's not usable, but hasn't melted down yet. Then have a reactor that is melted down and then becomes unusable.

So I can sacrifice this goal. If I need to do, if I need to keep this going, because it connects to the higher sets of goals, the way operators are thinking about this. And remember, this is the we're trying to look at what the domain is requiring. The way operators will think about this is going to depend upon how well they can understand what the goals and relationships are.

There is. If you're talking about what the mental model. Is for the operator. It's got to include this kind of man. Yeah. If they're to get it right. If you don't have this map, if the operator just has a map of supposedly operator just has a map of the core and how it works, that doesn't tell him what to do.

It doesn't answer the question. What are the consequences of doing these different things? It doesn't tell him how to act or her how to act. And so the whole point is that if you want the operators to be able to know what to do, they have to understand what this school means. Structure is at least enough in a way that they ended up sacrificing some goals to achieve others in the right way.

Obviously, if you end up with the operator. Who you confronted with the question? Is it more important to keep the reactor covered with water than it is to go and have dinner? And he says it's more important to go and have dinner. You have a serious problem. Everybody understands that. But the fact of the matter is that the TMI and Davis Bessey, and then virtually all the other reactors situations that we see, nobody has accused the operators of doing that.

They're all busy. They're all really trying to make things work. Nobody's misunderstanding that this is not a very important thing.

You might ask the question how do we map all this? How do we practically do this? And it's, it turns out of course, to be actually fairly difficult to get a good map. And  what you will see is that in most cases, the maps that people produce. Are the maps that they imagined that operators ought to have.

And they are usually consists of a few things, but nobody ever gets a sort of complete map of the whole reactor system. It's too complicated. It's too big. It's too hard to do. So if you've taken any one particular problem, you say, how do we deal with this problem over here? If you make a focus on that.

and you really look at it, you can say, Oh yes, I see it here. That there are these goals and they're in conflict here and there's a higher level goal. And I understand that these are the various kinds of means that are possible here and how they relate to these different things you can on demand. You can work this out in a particular age.

You can go into an area because I see. When I focused on that when I make my magnifying glass and I focus right at this area, I can see that this is what's going on, but nobody actually builds the whole model. All right. And the reason is because after a couple of steps in here, it becomes a manager.

It's just too complicated to actually make one of these. You can try, but you end up with reams of paper that nobody really understands. You end up with this kind of thing that looks like a big computer flow chart and it just, you don't really know what it actually means, but the fact is that in problem situations, if you look at particular difficult problems that people are confined with, that is if you say I know that you have.

TMI like thing. And I know you have a David Bessie type thing and a variety, any of others. If you look at these as examples of different kinds of things, you can build these things pretty well and you can actually get them. You can do them so well that you can build models of them in a computer and have the computer solve the problems of how to deal with this.

It's called the cognitive simulation. And one of the things that woods does. A year or two down the road here is build computer simulations of how people solve these kinds of problems and demonstrate that he gets the same solutions that the operators actually apply.

Okay. Now there's two kinds of ways that he proposes to fill this out for people I'm not going to try and fill it out. But you propose, there's two ways of looking. This is one of those one is he proposes this idea of evidence utilization.

And basically what this is looking at the way operators use data from the world. You make them, you let things happen. You watched them in simulators. You watched them during the day. But you watch them and you wait and you see how the operator,

how the operator looks into the world to use the evidence there. What is it that they're looking at? What are the things on the control panel that they use? What kinds of data are they looking at? And by doing this, you can begin to get a handle on their idea, particularly about some of the constraints on what it is that they can do.

Some ideas about what's actually going on as they search for data in the world, you watch them search for data in the world. Now we'll give you a clue as to what it is that they're trying to figure out anything here. The the idea of this is is partly driven by that need to find solutions to the problem, because.

Much of what people are doing in control room design is trying to fix the way evidence is displayed so that the evidence utilization becomes better. If you noticed one of the critiques of the checklists was that some checklists require you to look at the checklist and then look in one part of the control space, and then look at the checklist and at another part of the control space.

You sometimes have to be bouncing all over and there's no coherence to it. That's certainly true about how people had used utilized evidence in the power plants. And so there was this idea, gee, maybe we should put the evidence together. So you don't have to look over all these different places.

It's stuff like that. And these are quite logical ways of dealing with things. And  what's this telling you and what is it holding, but it's telling you to Jamie, got to try and look at how operator's really trying to use evidence and then make it easier for them to do that sort of thing.

Zero also point out that this is actually quite difficult to do for a variety of reasons, but that's one of the methods. The other method that they use is what is, I just love this because it's so obvious is what they call pragmatic reasoning.

And. What they're really doing is they're saying, look for situations in which people are trying to make Reese to reason about the way that things are working. Look at the kinds of reasoning that they're doing when they are trying to consider how to do the different things.  And he's looking at the, not just the way that they are reasoning about faucets and things like that, but the way that their reasoning about these higher goals.

It's really an interesting kind of question. What is pragmatic reasoning? Is there any other kind, is there non pragmatic reasoning? I don't think so. I don't think there's, I think pragmatic is supposed to mean come out connected to the world in some meaningful way. I don't really know very much about pragmatic reasoning and I'm not sure that anybody else really does either.

But the idea is in a way, if you think about this, the pragmatic reasoning and the evidence utilization are both ways of looking into the operator and what they are doing

for clues about what the demands are in the outside world. It is. At the end of this, although they really said we're mapping the demands, the cognitive demands of the domain. Hey, all right. They're trying to tell you about the requirements outside of the operator. In the end, they ended up going into the operators to try and figure out what's actually happening.

That is, they say the, after all the theory and all the talk, what they give you is the series of things which is go talk to the operators. Look at the documents, try and figure out what's important here. Ask anybody you can. And by the way, look at how the operators are trying to solve a problem and try and figure out what that isn't their reasoning about this.

I don't mean to make light of this. This is actually a fairly good idea but if you get past all the sort of elaborate technical language, this is what they're proposing that we do. And indeed They get into these kinds of elaborate sorts of things like does problem, does the problem-solver know that a requirement relation exists and is now relevant between units a and B or a post-condition goal process, alternative integral constraint, or other relationship, if for other reasons and needs to be done that beam must first be satisfied.

Therefore check if B is satisfied and so on. Yeah, we can understand that someone could possibly think about the world in that way. And yeah, maybe you could even build some models that way. That'd be really going to be able to dissect the world down to this point where we can make this complete map and model and answer all these questions.

And the answer is I think probably not. And that's one of the reasons why I think that this is not taken off in quite the way that was expected. Because in fact, although all of these ideas are quite reasonable and all of our thinking about them says, Oh, yes, I can see how you can do that.

And logical way you practice. It turns out to be really good, hard to do. In theory, it looks great. The university of Chicago, we used to have a saying, it's fine practice, but how does it work in theory? And. And in fact, this is one of the problems here is it's fine in theory, but it doesn't, it's not easy to see how it works in practice, but what does give us a, an observation and here in more of an empirical observation that really changes things.

And I think changes the world a little bit. And it's on page two 70 in the first full paragraph. And he starts to talk about some of the real problems that face people. And he says, look, he says, what a large number of interconnected parts is combined with another dimension of complexity, feminism that is this thing changing very rapidly, the disturbance management, cognitive situation.

Now, this is a really interesting idea. And what is this trying to get at here is this observation that operators can switch strategies. And is they can go from answering, try to answer these sorts of questions and other situations to say,

this is disturbance management and this is a completely different kind of activity and what they slipped us in here in this little paragraph with our homeless, with almost without any kind of introduction. And they tell us about disturbance management.

And then go on and talk about it just a little bit, but it turns out that this is really crucial disturbance management is what you do when you don't know what to do. Okay. There's a wonderful paper in problem-solving literature called problem solving what you do when you don't know what to do. Disturbance management is what you do.

When you don't know how to fix the underlying problem, but what you are trying to do is preserve the system long enough that you will have that chance. And anybody who works in the healthcare field knows exactly what disturbance management is. Disturbance management is when you start chest compressions.

When you start, when you are doing chest compressions, this is disturbance management. You've given up all the other diagnostic activities. And you've said, my primary goal right now is to keep. The little red things going round and round so that the big white thing up here continues to get enough oxygen that peoples, that's all you're saying.

You're not saying I know what the problem is. I can fix this though. Does it? This was this disturbance management is where you shift in this big way, a shift of activities that the people are doing from being in this. I'm trying to figure out what goes on the diagnostic. What is happening here?

How does this work what's happening? Too. I am trying to keep the system from essentially crashing or thyme, or just ending up in a disaster, sweaty, because if I don't do this, it won't matter. I'm not going to get the result and fix it in time. Yeah. It wasn't that going up to her really high level.

Yes. And what does the saying is that. There are times when remember, this is question about whether or not we're talking about a model of what's in the world or a model of what's in the head of the operator. And we fudge that and said, are we talking about domain models and mapping the complexities of the domain?

It sounds like it's outside the operator and so forth, but the point is that it can get so nasty in here, especially when things are changing rapidly, where there's lots of different components. That you can spend so much time trying to figure out what's happening in here, that the system dies nice while you're doing that.

And that one of the skills that operators is to essentially abandon those activities and change their operation, what it is that they are doing to a completely different kind of activity, which is keep it alive, keep it going. Now, this is a really kind of idea. It's much more, it's much more profound than it's given credit in the paper.

It doesn't, it appears just as a kind of, what another paragraph here, but the point is that the world, even the world of the nuclear power plant, which is pretty fixed, pretty static, pretty engineered. We understand it pretty well. Can get so confusing. What you have to do is stop trying to figure out what's going on and take steps that will simply preserve the pants.

You don't know, you can't answer the question. You are doing things differently, which is I'm trying to preserve this. And you're absolutely right. It is connected, but it may not be connected. I should say it is connected as a high-level goal. But what's happened is that the situation is such that people are down here somewhere in these lower level goals, trying to figure out relationships and solve problems and do diagnosis and all the rest of this stuff.

And during that time, the systems condition has gone from bad to worse. And now in order to achieve these high level goals, we have to stop doing that and do something quite different, even though the consequences of that may be very bad. No one likes to do chest compressions. All right. And it just, everybody hates doing chest compressions.

It's a sign of failure. I don't want to do it, for doing chest compressions. It's a bad thing. But when there's no circulation, you should be doing chest compressions and there's no other alternatives here. There is one guy who has an implantable ventricular assist device. There's no point in doing chest compression, doesn't help, but not shorter than that.

You're going to bounce out of whatever you're doing and go up and start managing the disturbance. And interestingly enough, we also observe now that one of the things that differs between really skilled and not so skilled operators is the really skilled operators seem to move to disturbance management at just the right point.

That is the person who starts doing the chest compressions at the right moment is a real expert. If you delay too long, you're a novice. And if you start to earlier analysis, it's getting the timing right.

But the point here is that that, what was the setting up for us as a kind of model about how to look at systems and how to try and evaluate them. And I think there's actually much more revealing than the paper seems to make it, which is that, although we're talking about the world and modeling characteristics of the world, we in fact find ourselves so overwhelmed with the complexity of that as analysts that we ended up going back and looking at operators and trying to figure out what it is that they are doing.

The way you understand the dynamics of these complicated worlds is not by starting from first principles and say the reactor core should be Marine covered. Will you do this as go out and study what operators do? Because they're the ones who are actually confirming this all the time. You need to be able to actually look at those people and they admit that's something that you have to do here.

You have to go and talk to these people and ask them what's going on. You can do some of this modeling stuff that they talk about and. You could actually talk that I think a lot about what the actual cognitive demands of the people are, but once you realize that there's this, you realize that not only are people doing this sort of thing, but they're supposed to keep doing it and reassessing it and updating it and becoming more aware of it all the time that it's not like I have an event.

And then I do this diagnostic process. And at the end of that, I fixed the problem. It's rather, I have an event. I started diagnostic process and then I also have to start handling the consequences of the event and start doing things and also keep track of stuff so that if something new comes along, I can recognize that and begin to step in and prepare.

And you're all operators. You all recognize that this is in fact what people I really do.

Okay. So  the, this idea here about how we get into the system and begin to model it, Woodson, whole Nagel, only England woods are proposing a very particular set of things. It sounds very good. We all buy it in theory, none of us know how to do it in practice, except for very specific cases, we can go in and look in a particular area.

And yeah, we can map some of this stuff out, but you know which area to look at well, we have to look at what the operators are using for evidence in the world and figure out what that means. And somehow they're supposed to get it, what their pragmatic reasoning is in a way  it's almost as if they've finessed the problem.

And given it back to us in another form, we started out trying to figure out how operators, reason about the system and what they're doing and all the rest of this stuff. And in the end, we get told that we should do some understanding about what the, how operators reasoning what's going on in the system.

I read this, it made me, I thought it was quite heavy reading, but a way. It made me, it reminded me analogy. I think Herbert Simon, what the end on the beach. And what they're saying is try to find a way to come onto the beach and to find an about the motivations, which make, which should decide which options, the address.

And then in fact, this whole process. So picture is very much in keeping with. Not Simon, but but a much deeper kind of model of how to work with the world, which goes way, way back, which is that by dividing the world up into its component pieces and looking at their relationships, I can build an adequate model.

The rules of will allow me to understand what's going on. It's a very Cartesian kind of idea. And it is. In fact, I think really well founded in Western thought about how systems work. What's interesting though, is if you think about the timing of this. They've had GMI actually, by now they've had Chernobyl, although that paper was written, unfortunately, but they've had, they had the journal that they're faced with a real problem.

The real problem is driving this. This is not thinking because we like to think about things. It's just thinking, because we just lost a multi-billion dollar plant. It's going to cost us $20 billion to clean it up. And the entire nuclear industry is on life support right now. And we have to figure out how to deal with it.

So there's lots and lots of people who are working on this and these guys are saying, Oh wait, we just remember that. We're trying to really figure out how to help people do these kinds of functions

just a little while after this.

Not actually more than a couple of years after this paper comes out.

We have an event that I think is really remarkable. It doesn't get a lot of coverage because it's happening in Canada, which is everyone knows it's just another us state that hasn't been figured it out.

But it turns out that in March of 1989, 10 years to the month after. TMI exactly. One decade later, you have the crash of dry near Ontario at Dryden of Aaron Cheerio folk, or F it's a two engine jet, which crashes on takeoff because there's ice on the wings and the. Plane kills. There are 23 people killed, including both pilots and the plane was so badly burned that neither the flight data recorder, nor the cockpit voice recorder are saveable. There's no data that automatic recordings of data don't exist.

They're wiped out by the fire very unusual event, but because this plane had just taken on a load of fuel before trying to take off, had lots of fuel on it. It's clear pretty quickly that it was that this was somehow related to ice on the wings. It turns out that the nature of the airfoil jets such that with this particular design, if you have even just a little bit of ice on the wing, you aren't going to be able to fly fast enough to be able to get the lift that you need to take off.

And because the ice on the wings, there was ice on weak and they tried to take off, but he never really did.

This is identified as a pilot error

by the captain of the flight. The first evaluation by Canadian aviation board says this is a clear case of pilot error. And the reason is because these things are completely unrelated. The pilot is the person who has the responsibility. Or evaluating ice on the surfaces of the airplane. The pilot is the one who makes the decision to take off.

If you did that, if you took off with ICER domains, it doesn't matter what else you say the pilot is to blame. All right. That's just basically the Canadian that and this is it's not just the Canadians that feel this way. A lot of people will say it's absolutely essential that the pilot be able to determine that the plane is airworthy before he takes off.

Cav says it's a pilot error story ends, except that a whole bunch of people are really upset with this because air Ontario is actually a small Bush aviation company that has joined with another company. And they have both been bought by air Canada.

interestingly enough. At the same time that this was happening, there was deregulation of the aviation market so that the companies could change around and change different flights and change with their charging. At the same time that this was going on, there was a cut back in the government.

And the amount of regulatory numbers of people that were involved in here and the Canadian aviation boards supervision stuff was limited. So you had both one explosive growth of the airline industry and a decreased size of the regulatory bodies. And there was real concern that this problem had something to do with the ongoing yeah.

Expansion of aviation and the problems with regulation and connect Canada is a parliamentary system. So there's no. There's no separate legislative investigative process. So they, the way that the parliamentary democracy is handled is by having a commission. And they took a former chief justice of the Supreme court of Canada justice  and they had him do a study and we'll share it ski study which started out after this is astonishing for a whole bunch of different reasons.

It looks at. Hundreds of thousands of pieces of evidence. It takes testimony under oath from a whole bunch of different people and hires a whole bunch of technical experts. It takes about two and a half years to complete the investigation that the commission complete the investigation and produces a four volume, actually a four printed volume report, which is huge.

It's in my office. You'd want to see it. That is the most exhaustive look out of aviation crash ever. This is an incredible, okay. And in part, the reason that it's such an incredible look is because when they crashed, they lost the flight data recorder and the cockpit voice recorder.

And there are no, there's no indication of what the pilots were saying to each other. And there's no indication of the way the plane was working. And this absence of data meant there was a kind of vacuum there about what was going on. And people had all sorts of concerns and considerations, and that together with this growth of stuff led to this kind of result, you've had a chance to read the report by Bob helm, who is brought in as a, an outside consultant by By the commission itself. And he writes what is by far the most cogent, most coherent and most interesting evaluation of the human factors of an aviation press that I know of. It's the most interesting because when he did this, he actually looked through all the different levels of the system, including the regulation.

And examined all the different kinds of problems that there were rather than just saying, why did the pilot take off? He said, why was the pilot put in the position of having to decide whether or not to take off with ice on the waves? And he comes up with a whole list of things, you've seen the list. But the interesting thing is that there are  there's at least 25 different contributing factors.

That include things like lack of regulatory oversight, poor management of the aircraft company, incomplete training, no having different versions of manuals based upon where you've been trained or where you were working, not having what's called a minimal equipment list. That is the list of things without which the plane cannot fly, allowing the plane to be dispatched, and then having arguments about refueling, taking it on, bringing off yours.

Let me just start a whole bunch of stuff. The bottom line is that all of these things ended up lifts. This who a pilot

sitting here on the runway

at about one o'clock in the afternoon was a big layer of ice on his wing.

And no choices left. In fact, he had been the day head on so badly. So many things had been screwed up so much stuff wasn't working. So many things were off that the only thing that he could do was either get the plane in the air or basically call it quits for the day. If he didn't fly, everybody was going to spend the night and Dryden, Ontario.

Your 50, some passengers. I'm wanting to look for hotel rooms in a town that does not have 50 hotel rooms. Everybody who's on this plane has somewhere to be. This is the beginning of the school holiday. It's like Sweden and the school holidays. Everybody's going. Okay. The other thing is that while he's sitting down here waiting to take off and the ice is accumulating.

Other things are happening. Assessment is being brought in. They get delayed for this. They get delayed for a whole bunch of different reasons. And each delay adds to the risk of trying to take off with ice on the wings. But the longer that he waits to take off the worst, the problem is getting, there's a huge incentive for him to get the plane in the air as quickly as possible.

The longer he waits down here, the worse the situation gets. And so what you have is the situation where if he stays on the ground, the risk is increasing. Maybe not linear like this. And there's a some sort of limit where we can do. And there's uncertainty about where these things actually are.

Where are we? I'm not sure, but we know that this is continuing to increase during this time. And he or the co-pilot of him together, make the decision to take off. The decision to take off was probably made quite a bit earlier. Probably wasn't made at the end of that rubbed. My they'd already essentially money taxied out there and got ready to go.

They were ready to go and they would have taken off, I think in all likelihood, irrespective of other considerations at that point, because they were trying to go, they had all these incentives to go and in face of all these incentives to go, they were getting virtually nothing that was helping them say, no, we shouldn't go at this point.

There's no organization in the company. There's no clear stuff. Everything that the pilot had tried to do it then frustrated he was getting no support from anybody. Now, how I'm writing his review is extremely critical of all these points. And then concludes something that I think is a hard to fathom, which is because he, he looks at all these many points, there's, he makes this big, long list of all these items.

You can read it. And then if he includes that you need no more CRM that is after listing all these things, all the bad stuff that the airline has done, all the screwball stuff, but he concludes it as you need more crew resource management training. Now you can ask yourself, why is it that he says more crew resource management trainings necessary?

It's because he invented crew resource. He's the guy who developed it. And he's the guy who has this strong, I believe that you can get crews to work together so they can handle virtually any problem. You can make them strong. He's still back in this kind of idea about how do we prevent pilots from making errors.

What we really do is  actually make it so that they have the  fortitude

the willpower, if you will, the necessary. Something rather so that they can together decide not to take it. Okay. It's hard for me to square this with what the rest of the report is. I think he pushes CRM way too hard. He is still trying to find a way to escape how the pilot to escape the system.

But the real problem with this is that the system itself was an unsafe system and it was playing in an unsafe way. Was running an unsafe way. And the pilots were being constantly pressured to essentially fly in unsafe conditions. And this just is one of the unsafe conditions that turned out badly.

It's not like the flights the day before were all that much better. It's just that this particular combination of small things was sufficient to cause an accident. The solution to this problem. And my view is not to say more training for pilots. Okay. Pilots are extensively trained. They get a lot of that stuff.

These guys training might've been. Deficient there were problems because there'd been a strike that had some effect between these two companies, all that sort of stuff. I don't care about any of that, because in the end you still have a situation in which you can't turn the engine off to  because there's no way that the engine restarted afterwards.

And frankly, that has nothing to do with the pilots. And if you put pilots in that situation and bad things happen, it's really hard for me to save. We're going to train the pilots enough so that those sorts of things aren't.

What's interesting about this is when you look at this kind of an event, you realize that the world is not made up out of systems that look like three mile Island, that the nuclear power plant model, this model that we've been using up until now, the one that started everything, the thing that got us all going, we all acknowledged this the watershed of him.

Make the system to make us start looking seriously at the safety of these big systems. Everyone agrees. This is an incredible staff, but we also see that even in the case of something like a nuclear power plant, it's so complicated that we don't really know how to trace through and figure out what the operators are doing.

And now we've started looking at a system out here that doesn't have. I have these properties of a nuclear power plant. It has. A huge number of factors that are in flux and changing all the time. And a huge number of difficulties in that are not related to the physical construction of the plant or the third one dynamics of things.

But they're related to people doing jobs in different ways, trying to get different kinds of goals met and trying to achieve different things. This problem up here turns out to be. Related, mostly to labor negotiations. How does that relate to the three mile Island? You don't see any of that there, it could never enter into the three mile Island question, but here, all of a sudden labor negotiations and labor strife becomes a big part of this accident scenario.

And what this has done is in some ways, please taken on a nice, safe world that we had in, in March of 79.

the three mile Island nuclear power plant world. And it's gotten us out to this world. It's just much, much less easy to understand much, much more fuzzy boundaries, not the kind of mechanical reasoning that we could go through here. A lot harder to talk about whose goals and what goals it becomes a big mess.

The big difference between these two is really that, and this is I think, shown in harmonics report. The big difference here is that by the time you get to March 89 and you have this accident, it's possible for everybody to understand this as a systems accident, this is a systems accident system.

Fail this. What is an operator error

somewhere in this 10 years, between March of 79 and March of 89, we went from a world where operators made errors to a world where we recognized that systems have failure modes that are. That include people on their actions, but it's all the people everywhere. It's the mechanics, it's the managers, it's the union representatives.

It's the regulators that kind of view of the system as the cause of the, that didn't exist in 1979. Progress that we've made in this period of time is to go from a model of operator errors, isolated failures. You couldn't understand what was going on. You did the wrong thing to recognizing that what we really have in the settings.

Is a systems accident, assistance accident. That's created by all these things coming together and people being unable to resolve it for it.

It changes the modeling problem that we had because as long as we were dealing with operator error and these control rooms and these fairly flat plants, we've been talking a lot about building all these elaborate models and talking about goals and needs and all that sort of stuff. That all seemed very plausible.

But when we get out here and we start to look at this. It starts to fall apart. We're having a hard time figuring it out. What is the information that we should add? Should we should give to the pilots? What information displayed you want them to have the billing information display? What is it that you're going to give to the company so that they can understand these relationships?

It's really hard to do. If you under, if you understand this event, this you're looking at all the factors that contribute to the pilot, making this decision, but you really find it hard if you read the full report or even Henry's view of it, you really find it hard to say, gee, if I just had a little bit more crew resource management training, I would have been able to avoid this accident.

The idea of blame and train. Which was so much part of this that we blame the operators first and we train them so that they will never do this. Again. This is gone here. It's not a satisfactory answer. In fact, what happens is that Canadians aviation, where tries to advance this answer and gets shot down.

They say it's a case of pilot error and the story. They tried, the old blame and train thing. Doesn't fly literally. And you get this big commission study. The changes. And this is really the onset of the kind of idea of system error.

Now we're pretty close to the end of our time. I don't want to spend a lot of time talking about the other papers here. I do want to mention just briefly a couple of things in. Hudgins paper, hutches paper and Hutchins books are wonderful. If you can read Hutchins paper and get how it is, the troll brand Islanders navigate, it's really something that's very hard to do because we are map and compass people, but you can get a feeling in there at one point that he does actually get it.

When he says there's this point at which he realizes. That if he draws the lines to the navigator where they intersect the navigator will be able to see it, this idea that boat is essentially stationary and the world is moving around underneath us. His description of that is really powerful, but the real warning in this is something else.

And what's one of the things that we have to be very careful of. And it's one of the things that I think is the most dangerous for this particular group. And that's the reason that the paper's in here, because all of you virtually all, you are practitioners, but you're all planning to do your studies at cognitive tasks and cognitive work in your home domains.

And yeah. And Hutchins says something very interesting here. It says there's a real methodological bind here due to the fact that we as researchers use our culture's notion of motion, both to navigate ourselves and to understand how others navigate. Okay. The enterprise is clearly fraught with opportunities to misinterpret observations in biased descriptions.

This is a warning that you should take and put out in bold pipe printed out and put it above your bed at night, because this is the real danger for you who want, if you go to a forum topic for an area that foreign domain where you are not an expert. The chances that you're going to get it wrong are much less because you're going to actually have to find out how the operators view the system.

But when you go to the world that you already know, and you try to study it, you have in your head a model of how that world works. And as a consequence, it's very easy for you to slip your model in to what you think the operator's model is. What Hodgkins tells us here is that there are. Are something on the order of 80 or nine 90 years of study of the navigations of these people that get it wrong all the time, because people are unable to escape their own models of navigation and actually take on the position of a Micronesian navigator.

And in doing huh work studies, one of the crucial elements is to be able to do that switch, to put yourself essentially in the position of the person who is. Trying to confront those problems so that you can understand what those cognitive tasks really are. And he points out here that this is actually a very difficult task.

It's a much harder than people expect. And he says, as is the case with any truly expert performance in any culture, the experts themselves are often unable to specify just what it is they do when they are, while they are performing, doing the task. And explaining what one is doing required quite different ways of thinking.

And this is especially true for us in medicine. And it's one of the major problems with healthcare studies, docs, nurses, pharmacists are all are just great talkers. They're facile, they're sophisticated, they've got great vocabulary. And so the, yeah, and they will, if you ask them why they are doing something, they will give you an explanation.

That explanation is usually completely unreal. Wait in to the actual why the cognitive functions of what they are doing when they're trying to solve that problem. So well, Hutchins is giving us a pretty big warning here, and it's something that you ought to take to heart because you're choosing to do your studies in a way that is actually the most difficult way, which is to stay in an area where you already have a great deal of domain knowledge.

It's not impossible. But it is much more hazardous because it's very easy to imagine that you understand the cognition of the people that you are studying, because it seems natural to you that things should be done that way. And it is only by doing this kind of research in an almost anthropologically.

The Hutchins is able to actually uncover this and figure out what

well with that warning, we come to the end of this first phase of. How is it that we're going to look at operators and understand what their what's going on?  We saw on three mile Island and in Rasmussen, there was a kind of methodological way that we could do this. We looked at woods and whole name and we say, Oh, yes, they're onto something.

This is clearly what we want to do. It's very complicated. And at the end we realized, Ooh, it's actually maybe so complicated that may not actually be practical in those situations. We can look at some. But building whole models of systems is hard. And then we blew up the system by going from 1979 to 1989 and say, Oh, here's this really complicated system.

They said, this was a complex system. Now let me show you a really complex system. This one over here. Now that's complex. And if you step from here into 2009 or 2013, and you said, look at Madison, you realize that the complexity here is very likely to overwhelm us as we try to look. Different kinds of operations, natures of operators.

This is a real risk. It's a real problem with doing what we do. And it's one of the reasons that people get defeated or feel like they're defeated as they try to do these things, because the complexity of this tends to overwhelm us. The next session that we do is going to talk about. How are we to pursue this?

What how can we get into this world? It's clear that people are doing these things. It's really clear that we need to understand what's happening in operator's mindset. We have to get into that, to know what's going on, but it's unclear exactly how to do it. That formal mechanisms, words are important equals seem to suggest.

I don't think that's got much. We need to do something else now. And that's what we'll be pursuing in the next in the next seminar. Okay. Who has questions at time? The character

 Questions. Go ahead.  I'm probably slow, but I still see parallels and there's this paper from . And the second case with the driver and the approach to. Looking at systems by mapping out the terrain and also looking at the drivers between behind actions. I just, I see they, they range the system in different ways.

You need a wider range perhaps there that when you can look and fit into people's heads. So that's, again, going back to that analogy, you try to find out what the beaches and what the ad's motivations are. That's a way of. Of seeing possible system States.

Yes. Nothing wrong with what was in whom they were saying. I'm not, I didn't mean to suggest that they've said it incorrectly. I didn't need to say to them that they're wrong in any kind of formal way. To say that the complexity of the real world. Overwhelms our best intentions to make these clear and exhaustive kinds of models.

And although we end up trying to figure out a way to that will make things better for operators. We really ended up looking back at the operators even what's in holding. We'll admit that, although we were trying to model the demands, that domain puts on the operators. The information that we're going to try and get is by looking at the operators and trying to figure out what they are doing.

And there's a, it's not a complete modeling process that gets us someplace. I don't think that they're wrong in a theoretical sentence. I think they're wrong in a practical sense, because I think in fact that you can't do it every time people have tried to do it, they get either overwhelmed with a number of things that are there and they can't, they essentially run out of stock.

They can't go any further and they don't know what they bought. Or they ended up picking a particular area and working through it. But recognizing that they've only got some small subset of the system. And I think that this is the trend. This is the trade off that you're going to end up trying to make, as you try to study these areas, which is, I want to get a big enough picture that I've explained what's important in the system, but in order to do, to make it managing, to do the right sorts of things, I have to somehow.

Bomb bound what it is that I'm looking at in a way that lets me understand this particular area. I got to figure out how to look, what should I be spending my time on? Because otherwise I'm going to spend years and years building models of things that are changing so fast that in fact, the model that I've produced will be out of date by the time I've written it down.

So I think you're right that there's a strong connection. And I think it's clear that woods and whole Nagel, because they were still working on trying to figure it out. The nuclear power plant problem have tried very hard to set up a path on which you could get the good results there.

But in the meantime, we started having accidents in other places and other types of things that are pulling in sport into a future that looks quite different. And he, with you look at Chernobyl, you're going to suddenly realize that a lot of this stuff is going to become out of. Out of our reach, that we're never going to be able to get into this.

We're never going to be able to touch on it Chernobyl in 1986 and by 19. Yeah. And United starts safety culture is is directly the product that, the words first used there it's first described in relationship to Chernobyl all of the safety culture stuff comes out of Chernobyl.

And the reason that people talk about safety culture as they just can't figure out how the heck those guys could get into a situation. And they were doing what they were doing. It just seems inconceivable that these guys could be trying to operate that magic. That way. It's a Chernobyl is a complete revolution in the way that people start doing these things here, you have these reasonable operators trying to work on a plant that had fault in that it was broken.

They're trying to fix it in Chernobyl. You had people trying to do an experiment with a reactor. Running it in very unusual conditions that they actually do not truly understand. And because of the design of the reactor end up producing a runaway reaction, they get they get essentially a positive reactivity, coefficient, and nothing blows up.

And everybody afterwards is trying to figure out why the system could be left this way. The plans in. The people who developed the reactor, they understood that this was a possibility. They understood that it was a hazard. They were busy trying to get people to change the reactor so that it wouldn't have this particular patch problem just didn't get there in time.

So we're going to from three mile Island, what's going to happen is we may have this expansion outwards in all these different directions, topic, research, or crew resource management different kinds of safety culture. All this stuff is going to start. Exploding out here as people encounter new accidents and try and cope with the results of it.

One of the things that you're going to find though, is that most of the steps of improvement, most of the activities that go on are action driven. That is each one of these things as a kind of increment that gets pulled forward by the next accident that we have, the next accident is what shapes the way we do things.

Causes us to build new models and turn our attention in new directions. It's all accident driven, although we wish it were not. Although we all want to go into the world of cognitive systems engineering, where everybody thinks these things through from the beginning and builds nice systems in the end, it's all about the accidents.